{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598373728263",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING TEMPORAL DIFFRENCE LEARNING IN FROZEN LAKE (openai's gym environment) using q-learning (qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting gym\n  Downloading gym-0.17.2.tar.gz (1.6 MB)\n\u001b[K     |████████████████████████████████| 1.6 MB 13 kB/s \n\u001b[?25hRequirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym) (1.4.1)\nRequirement already satisfied: numpy>=1.10.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym) (1.18.3)\nCollecting pyglet<=1.5.0,>=1.4.0\n  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n\u001b[K     |████████████████████████████████| 1.0 MB 16 kB/s \n\u001b[?25hCollecting cloudpickle<1.4.0,>=1.2.0\n  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\nCollecting future\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[K     |████████████████████████████████| 829 kB 26 kB/s \n\u001b[?25hBuilding wheels for collected packages: gym, future\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650891 sha256=350afba66562fd108dbd1c13befabcc3a9f2c8a99ca410a9e5cf3dd42bf99bff\n  Stored in directory: /Users/user/Library/Caches/pip/wheels/48/bf/7c/44b1b8e4ad998fc48e31caedbb9e028351861b8d20632642bc\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=5c6fba1fc38ecf96d8a8747a77d0425d09360bdcee8810a31c1981e715d463b2\n  Stored in directory: /Users/user/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\nSuccessfully built gym future\nInstalling collected packages: future, pyglet, cloudpickle, gym\nSuccessfully installed cloudpickle-1.3.0 future-0.18.2 gym-0.17.2 pyglet-1.5.0\n"
    }
   ],
   "source": [
    "!python3 -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages gym for environment and performing trials, random for determining exploitation or exploration using episilon, and numpy for numerical computation in array-like manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random example from https://gym.openai.com/docs/ to test my gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define enironment for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get environment size for determining qtable. size is observable space and possible action is action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print (qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining our hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000 #number of times the episode/ trials are done.\n",
    "learning_rate = 0.7 #rate of update of action_space values in a state\n",
    "max_steps = 99 #max_number of step to take in environment before end of episode\n",
    "gamma = 0.95 #discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialising learning episodial learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon  = 1 # variable used in epsilon greedy algorithm\n",
    "max_epsilon = 1 # the ceiling value of epsilon\n",
    "min_epsilon = 0.01 # the floor value of epsilon \n",
    "epsilon_range = max_epsilon - min_epsilon # \n",
    "decay_rate = 0.005 # the power of e = 2.7182818285, natural number, by which we decrease epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializing the agent's accumulated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_rewards = []\n",
    "explore_exploit_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this block of code represents our agent performing actions in the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    state = env.reset() # we are starting from the left-uppermost position (i.e the beginning state of the frozen lake environment)\n",
    "    total_reward = 0 # the reward our agent accumulates over the 100000 episodes\n",
    "    dead = False\n",
    "    for step in range(max_steps):\n",
    "        num = random.random()\n",
    "        if num > epsilon: # condition to determin exploration or exploitation\n",
    "            action = np.argmax(qtable[state,:]) # exploitation\n",
    "            explore_exploit_data.append(\"exploiting\")\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            explore_exploit_data.append(\"exploring\")\n",
    "            # action = random.choice(qtable[state,:]) # exploration\n",
    "\n",
    "        new_state, reward, dead, info = env.step(action) # getting the result of our action. a new state, a reward, information on our health, and general debugging info\n",
    "\n",
    "        qtable[state,action] += learning_rate * (reward + gamma * max(qtable[new_state,:]) - qtable[state,action]) # the q-function for updating state value\n",
    "\n",
    "        total_reward += reward # rewards gotten in the episode\n",
    "        state = new_state # moving our agent to the newly acquired position or state\n",
    "\n",
    "        if dead == True: # if our agent happened to die, stop the episode \n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + epsilon_range * np.exp(-decay_rate * episode) # carrying out exponential decay on epsilon to encourage less exploration as more episodes pass\n",
    "    accumulated_rewards.append(total_reward) # keeps track of the reward accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n0.0\n1.0\n51077.0\n[[1.04895818e-01 1.03253230e-01 3.35242332e-02 9.47920824e-02]\n [6.51350787e-03 7.45885250e-03 1.96463395e-02 9.32722979e-02]\n [4.75110578e-02 7.75691702e-02 1.19083434e-01 1.21872452e-02]\n [3.62453822e-03 7.29698923e-03 7.38543313e-04 8.40236062e-02]\n [1.02641332e-01 2.11684420e-02 4.50628767e-02 3.72069289e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [3.06387715e-04 1.52181824e-03 4.18808694e-02 4.82170981e-04]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [2.21515356e-03 2.85133816e-03 4.42928449e-02 2.79052876e-01]\n [8.37041974e-02 7.32041424e-01 7.49398125e-02 5.66369136e-02]\n [5.61541884e-01 4.06884399e-03 1.15042956e-02 1.03213361e-03]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [4.74156039e-02 4.08059519e-02 5.70406523e-01 1.70412410e-01]\n [2.21869809e-01 8.46415876e-01 1.77112876e-01 8.83664338e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
    }
   ],
   "source": [
    "for i in range(0,len(accumulated_rewards),100):\n",
    "    print (accumulated_rewards[i])\n",
    "print (sum(accumulated_rewards)) # to observe performance of agent for the hyperparameters we used.\n",
    "print (qtable) # observe the value of state (the table can help tell us the state to be in a point in time and how to navigate the game to obtain maximium sccores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysing exploit to explore ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(explore_exploit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 0\ncount        36127\nunique           2\ntop     exploiting\nfreq         31309",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>36127</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>exploiting</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>31309</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent exploited 31309 time while exploring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing: navigating the frozen lake environment with the goal and start state set at 15 and 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "eureka, agent has won!! in steps53\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\neureka, agent has won!! in steps51\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\neureka, agent has won!! in steps23\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\neureka, agent has won!! in steps28\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\neureka, agent has won!! in steps18\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\neureka, agent has won!! in steps16\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\neureka, agent has won!! in steps76\n  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n"
    }
   ],
   "source": [
    "accumulated_rewards = [] # resetting our accumulate rewards\n",
    "env.reset() # resetting our environment\n",
    "for i in range(10): # performing 10 episodes using qtable\n",
    "    state = env.reset()\n",
    "    dead = False\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        new_state, reward, dead, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        if new_state == 15: # if goal state is reached\n",
    "            print (\"eureka, agent has won!! in steps\" + str(step+1) )\n",
    "        if dead == True:\n",
    "            env.render() # prints the environment if the agent dies\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "    accumulated_rewards.append(total_reward)"
   ]
  }
 ]
}