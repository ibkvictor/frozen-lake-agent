{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598373728263",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING TEMPORAL DIFFRENCE LEARNING IN FROZEN LAKE (openai's gym environment) using q-learning (qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting gym\n  Downloading gym-0.17.2.tar.gz (1.6 MB)\n\u001b[K     |████████████████████████████████| 1.6 MB 13 kB/s \n\u001b[?25hRequirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym) (1.4.1)\nRequirement already satisfied: numpy>=1.10.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym) (1.18.3)\nCollecting pyglet<=1.5.0,>=1.4.0\n  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n\u001b[K     |████████████████████████████████| 1.0 MB 16 kB/s \n\u001b[?25hCollecting cloudpickle<1.4.0,>=1.2.0\n  Downloading cloudpickle-1.3.0-py2.py3-none-any.whl (26 kB)\nCollecting future\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[K     |████████████████████████████████| 829 kB 26 kB/s \n\u001b[?25hBuilding wheels for collected packages: gym, future\n  Building wheel for gym (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650891 sha256=350afba66562fd108dbd1c13befabcc3a9f2c8a99ca410a9e5cf3dd42bf99bff\n  Stored in directory: /Users/user/Library/Caches/pip/wheels/48/bf/7c/44b1b8e4ad998fc48e31caedbb9e028351861b8d20632642bc\n  Building wheel for future (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=5c6fba1fc38ecf96d8a8747a77d0425d09360bdcee8810a31c1981e715d463b2\n  Stored in directory: /Users/user/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\nSuccessfully built gym future\nInstalling collected packages: future, pyglet, cloudpickle, gym\nSuccessfully installed cloudpickle-1.3.0 future-0.18.2 gym-0.17.2 pyglet-1.5.0\n"
    }
   ],
   "source": [
    "!python3 -m pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages gym for environment and performing trials, random for determining exploitation or exploration using episilon, and numpy for numerical computation in array-like manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random example from https://gym.openai.com/docs/ to test my gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define enironment for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get environment size for determining qtable. size is observable space and possible action is action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print (qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining our hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100000 #number of times the episode/ trials are done.\n",
    "learning_rate = 0.7 #rate of update of action_space values in a state\n",
    "max_steps = 99 #max_number of step to take in environment before end of episode\n",
    "gamma = 0.95 #discount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialising learning episodial learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon  = 1 # variable used in epsilon greedy algorithm\n",
    "max_epsilon = 1 # the ceiling value of epsilon\n",
    "min_epsilon = 0.01 # the floor value of epsilon \n",
    "epsilon_range = max_epsilon - min_epsilon # \n",
    "decay_rate = 0.005 # the power of e = 2.7182818285, natural number, by which we decrease epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initializing the agent's accumulated reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_rewards = []\n",
    "explore_exploit_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this block of code represents our agent performing actions in the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    state = env.reset() # we are starting from the left-uppermost position (i.e the beginning state of the frozen lake environment)\n",
    "    total_reward = 0 # the reward our agent accumulates over the 100000 episodes\n",
    "    dead = False\n",
    "    for step in range(max_steps):\n",
    "        num = random.random()\n",
    "        if num > epsilon: # condition to determin exploration or exploitation\n",
    "            action = np.argmax(qtable[state,:]) # exploitation\n",
    "            explore_exploit_data.append(\"exploiting\")\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            explore_exploit_data.append(\"exploring\")\n",
    "            # action = random.choice(qtable[state,:]) # exploration\n",
    "\n",
    "        new_state, reward, dead, info = env.step(action) # getting the result of our action. a new state, a reward, information on our health, and general debugging info\n",
    "\n",
    "        qtable[state,action] += learning_rate * (reward + gamma * max(qtable[new_state,:]) - qtable[state,action]) # the q-function for updating state value\n",
    "\n",
    "        total_reward += reward # rewards gotten in the episode\n",
    "        state = new_state # moving our agent to the newly acquired position or state\n",
    "\n",
    "        if dead == True: # if our agent happened to die, stop the episode \n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + epsilon_range * np.exp(-decay_rate * episode) # carrying out exponential decay on epsilon to encourage less exploration as more episodes pass\n",
    "    accumulated_rewards.append(total_reward) # keeps track of the reward accumulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0\n5.0\n[[3.15940148e-01 4.18405719e-02 4.19075565e-02 3.75728520e-02]\n [1.35689547e-02 1.21687754e-02 3.11019236e-02 3.98573907e-02]\n [1.51562674e-02 5.15704281e-02 2.24772804e-02 2.48552472e-02]\n [8.67740647e-03 1.87677661e-02 1.51824660e-02 3.66605297e-02]\n [4.97581287e-01 4.51365130e-02 2.14062876e-02 7.49869698e-02]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [4.04709297e-03 5.06313211e-06 1.66656791e-02 6.58111640e-06]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [8.96713310e-02 5.20557213e-01 2.50197900e-02 5.58682671e-01]\n [5.15343344e-02 7.62763507e-01 4.96532289e-02 1.13928675e-02]\n [2.20495454e-01 1.58280183e-02 1.17006524e-02 7.19475287e-03]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n [1.03689850e-01 5.98668397e-01 6.77280587e-01 6.98557600e-02]\n [1.84616156e-01 2.46293744e-01 8.98023110e-01 1.46883197e-01]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
    }
   ],
   "source": [
    "for i in range(0,len(accumulated_rewards),10000):\n",
    "    print (accumulated_rewards[i])\n",
    "print (sum(accumulated_rewards)) # to observe performance of agent for the hyperparameters we used.\n",
    "print (qtable) # observe the value of state (the table can help tell us the state to be in a point in time and how to navigate the game to obtain maximium sccores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analysing exploit to explore ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(explore_exploit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 0\ncount      3757279\nunique           2\ntop     exploiting\nfreq       3717288",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3757279</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>exploiting</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>3717288</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent exploited 31309 time while exploring "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing: navigating the frozen lake environment with the goal and start state set at 15 and 0 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "eureka, agent has won!! in steps15\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\neureka, agent has won!! in steps21\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\neureka, agent has won!! in steps48\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\neureka, agent has won!! in steps23\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Right)\nSFFF\nFHF\u001b[41mH\u001b[0m\nFFFH\nHFFG\neureka, agent has won!! in steps17\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n"
    }
   ],
   "source": [
    "accumulated_rewards = [] # resetting our accumulate rewards\n",
    "env.reset() # resetting our environment\n",
    "for i in range(10): # performing 10 episodes using qtable\n",
    "    state = env.reset()\n",
    "    dead = False\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(qtable[state,:])\n",
    "\n",
    "        new_state, reward, dead, info = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        if new_state == 15: # if goal state is reached\n",
    "            print (\"eureka, agent has won!! in steps\" + str(step+1) )\n",
    "        if dead == True:\n",
    "            env.render() # prints the environment if the agent dies\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "    accumulated_rewards.append(total_reward)"
   ]
  }
 ]
}